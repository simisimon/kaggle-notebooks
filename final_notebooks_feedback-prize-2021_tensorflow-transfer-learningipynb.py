#!/usr/bin/env python
# coding: utf-8

# <div class='alert alert-info' style='background-color: #ffe4c2; color: #000000;border: 0px; font-size: 16px; padding: 0px 0px 0px 13px;'>
#     
# <div class='alert alert-info' style='text-align: center; background-color: #eb8909; color: #ffffff; border: 0px; font-size: 16px; padding: 1px 1px 1px 15px;'><h1 style='color: #ffffff'>Tensorflow - Transfer Learning</h1>
#     <h3 style='color: #ffe4c2'>- Image Classification with EfficientNet -</h3>
# </div>
# 
# 
# <h4>So, you're an aspiring data scientist and you just joined the latest computer vision comp on Kaggle and you're hungry for gold ..</h4>
# 
# <h4>Then you realize that you don't have time to gather millions of images nor the compute resources to scratch build the world's finest custom image classifier ..</h4>
# 
# <h4>Do not fear .. transfer learning <b><i>IS</i></b> all you need!</h4>
# 
# 1. Build a simple CNN<br>
# 2. Use transfer learning to build a better CNN<br>
# 3. Rejoice!<br>
#     
# 
# - The problem we need to solve is a <u>binary image classification task</u>.<br>
# - The goal is to demonstrate the ease of using transfer learning with Tensorflow to create a solution.<br>
# - For the purposes of this notebook, we'll use this fantastic Cat/Dog dataset: https://www.kaggle.com/datasets/tongpython/cat-and-dog  <br>
# - Thanks https://www.kaggle.com/tongpython for posting the dataset.

# # What is transfer learning ?

# <div class='alert alert-info' style='background-color: #ffe4c2; color: #000000; border: 0px; font-size: 16px; padding: 0px 0px 0px 13px;'>
# <div class='alert alert-info' style='background-color: #eb8909; color: #ffe4c2; border: 0px; font-size: 16px; padding: 1px 1px 1px 15px; height: 5px;'>
# </div>
# 
# - A machine learning method where a model trained for one task is reused as the starting point for another task.
# - More about transfer learning theory here -> https://en.wikipedia.org/wiki/Transfer_learning
# - tensorflow.org tutorial -> https://www.tensorflow.org/tutorials/images/transfer_learning

# ![transfer_learning.PNG](attachment:4d9a07d0-f0b8-49e8-bf81-76ca9bc4a43a.PNG)

# # Setup

# <div class='alert alert-info' style='background-color: #ffe4c2; color: #000000; border: 0px; font-size: 16px; padding: 0px 0px 0px 13px;'>
# <div class='alert alert-info' style='background-color: #eb8909; color: #ffe4c2; border: 0px; font-size: 16px; padding: 1px 1px 1px 15px; height: 5px;'>
# </div>
# 
# - We just need to import a couple things and set the TF log level.

# In[ ]:


import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # Set the log level to keep the warnings down

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt


# # Load data

# <div class='alert alert-info' style='background-color: #ffe4c2; color: #000000; border: 0px; font-size: 16px; padding: 0px 0px 0px 13px;'>
# <div class='alert alert-info' style='background-color: #eb8909; color: #ffe4c2; border: 0px; font-size: 16px; padding: 1px 1px 1px 15px; height: 5px;'>
# </div>
# 
# * Since the images are stored in sub directories with the class name as the folder name, we can use the handy Keras <b><i>image_dataset_from_directory()</i></b> function.
#     * ../input/cat-and-dog/training_set/training_set/cats/
#     * ../input/cat-and-dog/training_set/training_set/dogs/
#     * ../input/cat-and-dog/test_set/test_set/cats/
#     * ../input/cat-and-dog/test_set/test_set/dogs/
#     
#     
# * We'll load training and validation data into separate BatchDataset objects.
# * Keras will use directory names as class labels with the <b><i>labels='inferred'</i></b> parameter (or we could specify them manually).

# In[ ]:


# Load the training dataset
train_ds = tf.keras.preprocessing.image_dataset_from_directory(
    '../input/cat-and-dog/training_set/training_set/',
    labels = 'inferred',
    batch_size = 64,
    image_size = (224,224),
    shuffle = True,
    seed = 82
)


# In[ ]:


# Load the test dataset
test_ds = tf.keras.preprocessing.image_dataset_from_directory(
    '../input/cat-and-dog/test_set/test_set/',
    labels = 'inferred',
    batch_size = 64,
    image_size = (224,224),
    shuffle = True,
    seed = 82,
)


# <div class='alert alert-info' style='background-color: #ffe4c2; color: #000000; border: 0px; font-size: 16px; padding: 0px 0px 0px 13px;'>
# <div class='alert alert-info' style='background-color: #eb8909; color: #ffe4c2; border: 0px; font-size: 16px; padding: 1px 1px 1px 15px; height: 5px;'>
# </div>
# 
# - The dataset contains about 10k images of dogs and cats already split into train/test directories.
# - Due to the relative complexity of the images, 5k per class is a low number of images for a CNN to learn on.
# - Let's take a look at some of the data.

# In[ ]:


# Class name list generated by image_dataset_from_directory()
train_ds.class_names


# In[ ]:


# Plot out a few of the images and their labels so we can see what we're working with.
plt.figure(figsize=(10, 10))

for images, labels in train_ds.take(1):
    for i in range(9):
        ax = plt.subplot(3, 3, i + 1)
        plt.imshow(images[i].numpy().astype("uint8"))
        plt.title(train_ds.class_names[labels[i]])
        plt.axis("off")


# # Build model

# <div class='alert alert-info' style='background-color: #ffe4c2; color: #000000; border: 0px; font-size: 16px; padding: 0px 0px 0px 13px;'>
# <div class='alert alert-info' style='background-color: #eb8909; color: #ffe4c2; border: 0px; font-size: 16px; padding: 1px 1px 1px 15px; height: 5px;'>
# </div>
# 
# - We can make a simple CNN using the Keras Functional API.
# - This is a binary classification problem, so we'll set a final Dense layer with 1 output and a sigmoid activation function.
# - I won't go into detail here. I'll assume you have a working knowledge of CNNs. For now, we'll just put a basic network in-place.

# In[ ]:


# Input layer
inputs = keras.Input(shape=[224,224,3])

# Add some Conv2D and BatchNorm layers
x = layers.Conv2D(128, 3)(inputs)
x = layers.BatchNormalization()(x)
x = keras.activations.relu(x)
x = layers.MaxPool2D()(x)
x = layers.Dense(64, activation='relu')(x)

x = layers.Flatten()(x)

# Make a binary output layer with sigmoid activation
outputs = layers.Dense(1, activation='sigmoid')(x)

# Create the model
model = keras.Model(inputs=inputs, outputs=outputs)


# In[ ]:


model.summary()


# <div class='alert alert-info' style='background-color: #ffe4c2; color: #000000; border: 0px; font-size: 16px; padding: 0px 0px 0px 13px;'>
# <div class='alert alert-info' style='background-color: #eb8909; color: #ffe4c2; border: 0px; font-size: 16px; padding: 1px 1px 1px 15px; height: 5px;'>
# </div>
#     
# * Of course, this is a simple model for brevity. A real world model would likely contain many more convolution and pooling layers .. but this will do for now.<br>
#     
# * Let's specify a loss function and an optimizer for training. We'll also designate a metric to use.

# In[ ]:


# Use BinaryCrossEntropy as a loss function since this is a binary classification problem
loss = keras.losses.BinaryCrossentropy()

# Use Adam optimizer with default learning rate
optimizer = keras.optimizers.Adam()

# The metric to monitor during training.
metrics = ['accuracy']


# # Compile and Fit

# <div class='alert alert-info' style='background-color: #ffe4c2; color: #000000; border: 0px; font-size: 16px; padding: 0px 0px 0px 13px;'>
# <div class='alert alert-info' style='background-color: #eb8909; color: #ffe4c2; border: 0px; font-size: 16px; padding: 1px 1px 1px 15px; height: 5px;'>
# </div>
# 
# - Once we're satisified we've created a reasonably good network, we'll go ahead and compile and fit it on the data.

# In[ ]:


model.compile(optimizer=optimizer, loss=loss, metrics=metrics)

model.fit(train_ds, epochs=10, batch_size=64, verbose=1)


# # Evaluate

# <div class='alert alert-info' style='background-color: #ffe4c2; color: #000000; border: 0px; font-size: 16px; padding: 0px 0px 0px 13px;'>
# <div class='alert alert-info' style='background-color: #eb8909; color: #ffe4c2; border: 0px; font-size: 16px; padding: 1px 1px 1px 15px; height: 5px;'>
# </div>
# 
# - The accuracy is looking pretty good on each epoch. Let's evaluate it on the test set.
# - The test images haven't been seen during training, so we don't need to be concerned with data leaks.

# In[ ]:


model.evaluate(test_ds, batch_size=64, verbose=2)


# <div class='alert alert-info' style='background-color: #ffe4c2; color: #000000; border: 0px; font-size: 16px; padding: 0px 0px 0px 13px;'>
# <div class='alert alert-info' style='background-color: #eb8909; color: #ffe4c2; border: 0px; font-size: 16px; padding: 1px 1px 1px 15px; height: 5px;'>
# </div>
#     
# - The overall accuracy of 65% isn't bad for a single model with very few images, epochs .. augments .. and no cross validation. Ok, maybe it's not quite a gold medal winning solution.<br>
# - We can make it better!

# # Transfer learning to the rescue !

# <div class='alert alert-info' style='background-color: #ffe4c2; color: #000000; border: 0px; font-size: 16px; padding: 0px 0px 0px 13px;'>
# <div class='alert alert-info' style='background-color: #eb8909; color: #ffe4c2; border: 0px; font-size: 16px; padding: 1px 1px 1px 15px; height: 5px;'>
# </div>
# 
# * One method to overcome the relatively low number of images is to use transfer learning to 'transfer knowledge' to our model.
#     
# * These models have been trained on millions of images and can be easily loaded using the [keras.applications ](http://https://keras.io/api/applications/) API.
#     - EfficientNet
#     - ResNet
#     - VGG
#     - and many more ..
#     
#     
# * Let's load EfficientNetB0 and use its weights as our base model, although any of the keras.applications can be used.
# * Once loaded, we'll <u>freeze the base model's weights</u>, so they don't get updated during training.

# In[ ]:


# Load the base model
base_model = tf.keras.applications.EfficientNetB0()

# Freeze the existing layers
base_model.trainable = False


# # Build another model

# <div class='alert alert-info' style='background-color: #ffe4c2; color: #000000; border: 0px; font-size: 16px; padding: 0px 0px 0px 13px;'>
# <div class='alert alert-info' style='background-color: #eb8909; color: #ffe4c2; border: 0px; font-size: 16px; padding: 1px 1px 1px 15px; height: 5px;'>
# </div>
#   
# * Now that the base model is loaded and frozen, we can add our input and output layers.
# * The output layer will be a Dense layer with sigmoid activation.

# In[ ]:


# Copy the input layer from the pretrained model to use as our input layer
inputs = base_model.layers[0].input

# Get all base model layers except the last two
outputs = base_model.layers[-2].output

# Add our final classification (output) layer
outputs = layers.Dense(1, activation='sigmoid')(outputs)

# Create the model
model2 = keras.Model(inputs=inputs, outputs=outputs)


# In[ ]:


# Commenting the model summary out because it's looooonng and doesn't look great in the saved notebook.
# model2.summary()


# <div class='alert alert-info' style='background-color: #ffe4c2; color: #000000; border: 0px; font-size: 16px; padding: 0px 0px 0px 13px;'>
# <div class='alert alert-info' style='background-color: #eb8909; color: #ffe4c2; border: 0px; font-size: 16px; padding: 1px 1px 1px 15px; height: 5px;'>
# </div>
#     
# - Woah .. that's a big model! (uncomment the summary to see it).<br>
# - We can see that EfficientNetB0 is sandwiched nicely between our input and output layers.<br>
# - The only trainable parameters are those in the final classification layer, which has a single output.

# In[ ]:


# Use the same loss, opt and metrics
loss = keras.losses.BinaryCrossentropy()
optimizer = keras.optimizers.Adam()
metrics = ['accuracy']


# # Compile and Fit

# <div class='alert alert-info' style='background-color: #ffe4c2; color: #000000; border: 0px; font-size: 16px; padding: 0px 0px 0px 13px;'>
# <div class='alert alert-info' style='background-color: #eb8909; color: #ffe4c2; border: 0px; font-size: 16px; padding: 1px 1px 1px 15px; height: 5px;'>
# </div>
# 
# - Using the same loss and optimizer, let's compile and fit the new model.

# In[ ]:


model2.compile(optimizer=optimizer, loss=loss, metrics=metrics)
model2.fit(train_ds, epochs=10, batch_size=64, verbose=1)


# In[ ]:


model2.evaluate(test_ds, batch_size=64, verbose=1)


# <div class='alert alert-info' style='background-color: #ffe4c2; color: #000000; border: 0px; font-size: 16px; padding: 0px 0px 0px 13px;'>
# <div class='alert alert-info' style='background-color: #eb8909; color: #ffe4c2; border: 0px; font-size: 16px; padding: 1px 1px 1px 15px; height: 5px;'>
# </div>
#     
# - This result is <b>much better</b> .. 99% accuracy! It's probably overfitting somewhat, but you get the idea.
# - More epochs and some augmentations would make this a medal-zone solution for sure!

# # Review

# <div class='alert alert-info' style='background-color: #ffe4c2; color: #000000; border: 0px; font-size: 16px; padding: 0px 0px 0px 13px;'>
# <div class='alert alert-info' style='background-color: #eb8909; color: #ffe4c2; border: 0px; font-size: 16px; padding: 1px 1px 1px 15px; height: 5px;'>
# </div>
# 
# - In this notebook, we took a quick look at what transfer learning actually is.
#     
# - We used Tensorflow 2 to load an image dataset and fit a simple CNN for multiclass classification on it. 
#     
# - Then we applied simple transfer learning techniques to embed a pretrained EfficientNet model into a second CNN. 
#     
# - Finally, we compared the evaluation results to demonstrate differences in accuracy.

# # Conclusion

# <div class='alert alert-info' style='background-color: #ffe4c2; color: #000000; border: 0px; font-size: 16px; padding: 0px 0px 0px 13px;'>
# <div class='alert alert-info' style='background-color: #eb8909; color: #ffe4c2; border: 0px; font-size: 16px; padding: 1px 1px 1px 15px; height: 5px;'>
# </div>
# 
# - When faced with an image classification problem where training images are few, Tensorflow makes it easy to incorporate transfer learning into your solution.<br>
# - By simplifying loading of pre-trained weights and interfacing with various architectures, we can quickly create accurate image classification models.<br>
#     
# - Here is a great video by "Python Engineer" on Transfer Learning with TF ->  https://www.youtube.com/watch?v=8cN0PiZQl18 .. I've learned a ton from this channel, give it a look.
#     
# - Thanks for your time, and happy learning!
