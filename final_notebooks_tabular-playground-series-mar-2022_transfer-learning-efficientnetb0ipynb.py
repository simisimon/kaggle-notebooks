#!/usr/bin/env python
# coding: utf-8

# In[ ]:


# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session


# <center style="font-family:verdana;"><h1 style="font-size:200%; padding: 10px; background: #6495ED;"><b style="color:white;">Transfer Learning</b></h1></center>

# "Transfer learning (TL) is a research problem in machine learning (ML) that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem. For example, knowledge gained while learning to recognize cars could apply when trying to recognize trucks."
# 
# https://en.wikipedia.org/wiki/Transfer_learning#:~:text=Transfer%20learning%20(TL)%20is%20a,when%20trying%20to%20recognize%20trucks.

# ![](https://miro.medium.com/max/1400/0*TAn7DD4VO4IfVOmY.png)medium.com

# #All script by David Roberts
# 
# https://www.kaggle.com/code/davidbroberts/tensorflow-transfer-learning/notebook

# #Set the TF log level.

# In[ ]:


#Code by David Roberts https://www.kaggle.com/code/davidbroberts/tensorflow-transfer-learning/notebook

import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # Set the log level to keep the warnings down

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt


# In[ ]:


#Code by David Roberts https://www.kaggle.com/code/davidbroberts/tensorflow-transfer-learning/notebook

# Load the training dataset
train_ds = tf.keras.preprocessing.image_dataset_from_directory(
    '../input/beach-sand-mineral-bsm/BSM/',
    labels = 'inferred',
    batch_size = 64,
    image_size = (224,224),
    shuffle = True,
    seed = 82
)


# In[ ]:


#Code by David Roberts https://www.kaggle.com/code/davidbroberts/tensorflow-transfer-learning/notebook

#We don't have Test. Save for the next time.

# Load the test dataset
#test_ds = tf.keras.preprocessing.image_dataset_from_directory(
 #   '../input/cat-and-dog/test_set/test_set/',
  #  labels = 'inferred',
   # batch_size = 64,
    #image_size = (224,224),
    #shuffle = True,
    #seed = 82,
#)


# In[ ]:


#Code by David Roberts https://www.kaggle.com/code/davidbroberts/tensorflow-transfer-learning/notebook

# Class name list generated by image_dataset_from_directory()
train_ds.class_names


# #Take a look at some of the data.

# In[ ]:


#Code by David Roberts https://www.kaggle.com/code/davidbroberts/tensorflow-transfer-learning/notebook

# Plot out a few of the images and their labels so we can see what we're working with.
plt.figure(figsize=(10, 10))

for images, labels in train_ds.take(1):
    for i in range(9):
        ax = plt.subplot(3, 3, i + 1)
        plt.imshow(images[i].numpy().astype("uint8"))
        plt.title(train_ds.class_names[labels[i]])
        plt.axis("off")


# #Build a model
# 
# "You can make a simple CNN using the Keras Functional API."
# 
# "This is a binary classification problem, so we'll set a final Dense layer with 1 output and a sigmoid activation function."
# 
# "I won't go into detail here. I'll assume you have a working knowledge of CNNs. For now, we'll just put a basic network in-place."
# 
# By David Roberts https://www.kaggle.com/code/davidbroberts/tensorflow-transfer-learning/notebook

# In[ ]:


#Code by David Roberts https://www.kaggle.com/code/davidbroberts/tensorflow-transfer-learning/notebook

# Input layer
inputs = keras.Input(shape=[224,224,3])

# Add some Conv2D and BatchNorm layers
x = layers.Conv2D(128, 3)(inputs)
x = layers.BatchNormalization()(x)
x = keras.activations.relu(x)
x = layers.MaxPool2D()(x)
x = layers.Dense(64, activation='relu')(x)

x = layers.Flatten()(x)

# Make a binary output layer with sigmoid activation
outputs = layers.Dense(1, activation='sigmoid')(x)

# Create the model
model = keras.Model(inputs=inputs, outputs=outputs)


# In[ ]:


#Code by David Roberts https://www.kaggle.com/code/davidbroberts/tensorflow-transfer-learning/notebook

model.summary()


# #Specify a loss function and an optimizer for training. Designate a metric to use.
# 
# Of course, this is a simple model for brevity. A real world model would likely contain many more convolution and pooling layers .. but this will do for now.
# 
# By David Roberts https://www.kaggle.com/code/davidbroberts/tensorflow-transfer-learning/notebook

# In[ ]:


#Code by David Roberts https://www.kaggle.com/code/davidbroberts/tensorflow-transfer-learning/notebook

# Use BinaryCrossEntropy as a loss function since this is a binary classification problem
loss = keras.losses.BinaryCrossentropy()

# Use Adam optimizer with default learning rate
optimizer = keras.optimizers.Adam()

# The metric to monitor during training.
metrics = ['accuracy']


# #Compile and Fit

# In[ ]:


#Code by David Roberts https://www.kaggle.com/code/davidbroberts/tensorflow-transfer-learning/notebook

model.compile(optimizer=optimizer, loss=loss, metrics=metrics)

model.fit(train_ds, epochs=10, batch_size=64, verbose=1)


# With GPU less than a minute!

# In[ ]:


#Code by David Roberts https://www.kaggle.com/code/davidbroberts/tensorflow-transfer-learning/notebook

#model.evaluate(test_ds, batch_size=64, verbose=2) #We don't have test. Saved it for the next time


# #Accuracy percent
# 
# The overall accuracy of 65% isn't bad for a single model with very few images, epochs .. augments .. and no cross validation. Ok, maybe it's not quite a gold medal winning solution.
# We can make it better!
# 
# By David Roberts https://www.kaggle.com/code/davidbroberts/tensorflow-transfer-learning/notebook

# #Transfer Learning
# 
# "One method to overcome the relatively low number of images is to use transfer learning to 'transfer knowledge' to our model."
# 
# These models have been trained on millions of images and can be easily loaded using the keras.applications API.
# 
# EfficientNet
# 
# ResNet
# 
# VGG
# 
# and many more ..
# 
# "Let's load EfficientNetB0 and use its weights as our base model, although any of the keras.applications can be used.
# Once loaded, we'll freeze the base model's weights, so they don't get updated during training."
# 
# By David Roberts https://www.kaggle.com/code/davidbroberts/tensorflow-transfer-learning/notebook

# In[ ]:


#Code by David Roberts https://www.kaggle.com/code/davidbroberts/tensorflow-transfer-learning/notebook

# Load the base model
base_model = tf.keras.applications.EfficientNetB0()

# Freeze the existing layers
base_model.trainable = False


# #Build another model
# 
# Now that the base model is loaded and frozen, we can add our input and output layers.
# 
# The output layer will be a Dense layer with sigmoid activation.
# 
# By David Roberts https://www.kaggle.com/code/davidbroberts/tensorflow-transfer-learning/notebook

# In[ ]:


#Code by David Roberts https://www.kaggle.com/code/davidbroberts/tensorflow-transfer-learning/notebook

# Copy the input layer from the pretrained model to use as our input layer
inputs = base_model.layers[0].input

# Get all base model layers except the last two
outputs = base_model.layers[-2].output

# Add our final classification (output) layer
outputs = layers.Dense(1, activation='sigmoid')(outputs)

# Create the model
model2 = keras.Model(inputs=inputs, outputs=outputs)


# In[ ]:


#Code by David Roberts https://www.kaggle.com/code/davidbroberts/tensorflow-transfer-learning/notebook

# Commenting the model summary out because it's looooonng and doesn't look great in the saved notebook.
# model2.summary()


# Woah .. that's a big model! (uncomment the summary to see it).
# 
# We can see that EfficientNetB0 is sandwiched nicely between our input and output layers.
# 
# The only trainable parameters are those in the final classification layer, which has a single output.
# 
# By David Roberts https://www.kaggle.com/code/davidbroberts/tensorflow-transfer-learning/notebook

# In[ ]:


#Code by David Roberts https://www.kaggle.com/code/davidbroberts/tensorflow-transfer-learning/notebook

# Use the same loss, opt and metrics
loss = keras.losses.BinaryCrossentropy()
optimizer = keras.optimizers.Adam()
metrics = ['accuracy']


# #Compile and Fit
# 
# Using the same loss and optimizer, let's compile and fit the new model.

# In[ ]:


#Code by David Roberts https://www.kaggle.com/code/davidbroberts/tensorflow-transfer-learning/notebook

model2.compile(optimizer=optimizer, loss=loss, metrics=metrics)
model2.fit(train_ds, epochs=10, batch_size=64, verbose=1)


# #With GPU it took less than a minute. Again!

# In[ ]:


#We don't have test. then I saved for next time.
model2.evaluate(train_ds, batch_size=64, verbose=1) #I changed test_ds (to train_ds) since we don't have test.


# This result is much better .. 0.1689% accuracy! It's probably overfitting somewhat, but you get the idea.
# More epochs and some augmentations would make this a medal-zone solution for sure!
# 
# By David Roberts https://www.kaggle.com/code/davidbroberts/tensorflow-transfer-learning/notebook

# When faced with an image classification problem where training images are few, Tensorflow makes it easy to incorporate transfer learning into your solution.
# 
# By simplifying loading of pre-trained weights and interfacing with various architectures, we can quickly create accurate image classification models.
# 
# #Thanks you David Roberts for all the script
# 
# https://www.kaggle.com/code/davidbroberts/tensorflow-transfer-learning/notebook
